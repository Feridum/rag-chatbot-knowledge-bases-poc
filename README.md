# RAG Chat with S3 Vectors

A production-ready chatbot built with **Vercel AI SDK**, **AI Elements**, and **AWS Bedrock Knowledge Base** using S3 Vectors for RAG (Retrieval-Augmented Generation).

## Features

- ðŸ¤– **AI-Powered Chat** - Uses Claude 3.5 Sonnet via AWS Bedrock
- ðŸ“š **RAG Implementation** - Retrieves relevant context from S3 Vectors knowledge base
- ðŸŽ¨ **Beautiful UI** - Styled with AI Elements components
- âš¡ **Streaming Responses** - Real-time streaming with Vercel AI SDK
- ðŸ”„ **Real-time Context** - Automatically enhances queries with relevant document context

## Tech Stack

- **Frontend**: Next.js 16, React 19, TypeScript
- **AI SDK**: Vercel AI SDK with `useChat` hook
- **UI Components**: AI Elements (built on shadcn/ui)
- **Backend**: AWS Bedrock, S3 Vectors for vector storage
- **Styling**: Tailwind CSS v4

## Architecture

```
User Query â†’ Frontend (useChat) â†’ API Route â†’ Generate Embeddings (Bedrock) â†’ Query S3 Vectors â†’ Retrieve Documents (S3) â†’ Claude 3.5 Sonnet â†’ Stream Response
```

The chatbot directly queries S3 Vectors at runtime. The setup script uses a Bedrock Knowledge Base only for ingestion (chunking + embedding) and writes vectors to S3 Vectors.

## Prerequisites

1. **AWS Account** with access to:
   - AWS Bedrock (Claude models)
   - S3 Vectors
   - IAM for role creation

2. **AWS Credentials** configured

3. **Node.js** 18+ installed

## Setup

### 1. Clone and Install Dependencies

```bash
npm install
```

### 2. Configure AWS

Copy the example environment file for the app runtime:

```bash
cp .env.example .env.local
```

Edit [.env.local](.env.local) with your AWS credentials:

```env
AWS_REGION=us-east-1
AWS_ACCESS_KEY_ID=your_access_key
AWS_SECRET_ACCESS_KEY=your_secret_key

# S3 Vectors ARNs (generated by setup script)
S3_VECTORS_BUCKET_ARN=arn:aws:s3vectors:region:account:vector-bucket/name
S3_VECTORS_INDEX_ARN=arn:aws:s3vectors:region:account:index/bucket/index-name
S3_BUCKET_NAME=kb-documents

# Optional: for setup script
BEDROCK_KB_ROLE_ARN=arn:aws:iam::YOUR_ACCOUNT:role/YourRole
```

Create a separate environment file for setup/ingestion scripts (used by `npm run setup-kb`):

```bash
cp .env.example .env.aws
```

### 3. Set Up Knowledge Base (ingestion only)

Run the setup script to create your AWS infrastructure:

```bash
npm run setup-kb
```

This will create:
- S3 bucket for documents
- S3 Vectors bucket for embeddings
- Vector index
- Bedrock Knowledge Base (used only to run ingestion)
- Data source configuration (used only to run ingestion)

**Important**: After the script completes, add the generated ARNs to your [.env.local](.env.local) file:
- `S3_VECTORS_BUCKET_ARN`
- `S3_VECTORS_INDEX_ARN`
- `S3_BUCKET_NAME`

### 4. Upload Documents and Generate Embeddings

Put files into the local [documents/](documents/) directory and re-run the setup script to upload + ingest, or use the upload helper script.

**Option A (recommended)**: local documents folder

1. Add files to [documents/](documents/)
2. Run:

```bash
npm run setup-kb
```

This uploads new local files to S3 and starts an ingestion job.

**Option B**: upload + ingest manually

```bash
npx tsx scripts/upload-documents.ts
```

Requires these env vars in `.env.aws`:
- `S3_BUCKET_NAME`
- `KB_ID`
- `DATA_SOURCE_ID`

### 5. Run the Development Server

```bash
npm run dev
```

Open [http://localhost:3000](http://localhost:3000) to see your chatbot!

## Project Structure

```
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â””â”€â”€ chat/
â”‚   â”‚       â””â”€â”€ route.ts          # Chat API with RAG integration
â”‚   â”œâ”€â”€ page.tsx                  # Main chat interface
â”‚   â”œâ”€â”€ layout.tsx                # Root layout
â”‚   â””â”€â”€ globals.css               # Global styles
â”œâ”€â”€ components/
â”‚   â”œâ”€â”€ ai-elements/              # AI Elements components
â”‚   â””â”€â”€ ui/                       # Base UI components
â”œâ”€â”€ documents/                    # Local files to upload for ingestion
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ setup-knowledge-base.ts   # AWS infrastructure + upload + ingest
â”‚   â””â”€â”€ upload-documents.ts       # Upload local docs + start ingestion
â”œâ”€â”€ lib/
â”‚   â””â”€â”€ utils.ts                  # Utility functions
â””â”€â”€ .env.example                  # Environment variables template
```

## Key Components

### Chat API Route ([app/api/chat/route.ts](app/api/chat/route.ts))

- Receives messages from the frontend
- Generates embeddings for user queries using Bedrock Titan
- Queries S3 Vectors directly for similar documents
- Retrieves actual document content from S3
- Enhances user queries with retrieved context
- Streams responses from Claude 3.5 Sonnet

### Chat Interface ([app/page.tsx](app/page.tsx))

- Built with AI Elements components
- Uses `useChat` hook from AI SDK
- Features:
  - Message history
  - Streaming responses
  - Suggestion chips
  - Empty state with prompts
  - Responsive design

## AI Elements Components Used

- `Conversation` - Main chat container
- `ConversationContent` - Scrollable message area
- `ConversationEmptyState` - Welcome screen
- `Message` & `MessageContent` - Individual messages
- `MessageResponse` - Formatted message display
- `PromptInput` - Text input with submit button
- `Suggestions` - Quick action chips

## Configuration

### Embedding Model

The default embedding model is `amazon.titan-embed-text-v2:0` (1024 dimensions). To change:

1. Update `EMBEDDING_MODEL` in [.env.local](.env.local)
2. Update `dimension` in [scripts/setup-knowledge-base.ts](scripts/setup-knowledge-base.ts)

### LLM Model

Default model: `anthropic.claude-3-5-sonnet-20241022-v2:0`

To change, update the model in [app/api/chat/route.ts](app/api/chat/route.ts):

```typescript
const result = streamText({
  model: bedrock("your-model-id"),
  // ...
});
```

## Development

```bash
# Start dev server
npm run dev

# Build for production
npm run build

# Start production server
npm start

# Run linting
npm run lint
```

## Environment Variables

| Variable | Required | Description |
|----------|----------|-------------|
| `AWS_REGION` | Yes | AWS region for all services |
| `AWS_ACCESS_KEY_ID` | Yes | AWS access key |
| `AWS_SECRET_ACCESS_KEY` | Yes | AWS secret key |
| `S3_VECTORS_BUCKET_ARN` | Yes | S3 Vectors bucket ARN (from setup script) |
| `S3_VECTORS_INDEX_ARN` | Yes | S3 Vectors index ARN (from setup script) |
| `S3_BUCKET_NAME` | Yes | S3 bucket name for documents |
| `EMBEDDING_MODEL` | No | Bedrock embedding model (default: titan-embed-text-v2) |
| `BEDROCK_KB_ROLE_ARN` | For setup | IAM role ARN for infrastructure creation |
| `KB_ID` | For upload script | Knowledge Base ID (from setup output) |
| `DATA_SOURCE_ID` | For upload script | Data Source ID (from setup output) |

## Troubleshooting

### "S3 Vectors configuration missing"

Make sure these environment variables are set in your `.env.local`:
- `S3_VECTORS_BUCKET_ARN`
- `S3_VECTORS_INDEX_ARN`
- `S3_BUCKET_NAME`

### "Failed to retrieve context"

1. Verify S3 Vectors bucket and index are created
2. Check that embeddings have been stored in S3 Vectors
3. Ensure your IAM role/user has permissions for:
   - `s3vectors:Query`
   - `s3:GetObject`
   - `bedrock:InvokeModel`

### No results returned

Make sure you've:
1. Uploaded documents to S3
2. Generated embeddings for those documents
3. Stored embeddings in S3 Vectors with proper metadata (including s3Uri)

### Styling Issues

If AI Elements components don't render correctly:

```bash
# Reinstall dependencies
rm -rf node_modules package-lock.json
npm install

# Reinitialize AI Elements
npx ai-elements@latest init
```

## Learn More

- [Vercel AI SDK Documentation](https://sdk.vercel.ai/docs)
- [AI Elements Documentation](https://ai-elements.vercel.ai)
- [AWS Bedrock Documentation](https://docs.aws.amazon.com/bedrock/)
- [S3 Vectors Documentation](https://docs.aws.amazon.com/s3vectors/)

## License

MIT
